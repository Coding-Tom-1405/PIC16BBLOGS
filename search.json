[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/HW3/index.html",
    "href": "posts/HW3/index.html",
    "title": "Homework 3: Web Development",
    "section": "",
    "text": "In this blog post, we will learn how to create simple webapp via Flask, a web framework that provides useful tools and features that make creating web applications in Python a lot easier.\n\n\n\nPIC16B-HW3-PIC1.png\n\n\n\n\n\nPIC16B-HW3-PIC2.png\n\n\n\n\n\nPIC16B-HW3-PIC3.png"
  },
  {
    "objectID": "posts/HW1/index.html",
    "href": "posts/HW1/index.html",
    "title": "Homework 1: Database Visualization (Climate Change)",
    "section": "",
    "text": "In this blog, we will show how to work with databases in order to create interesting and interactive data graphics. We want to utilize databases when dealing with data sets that are incredibly large to the point of being impratical or inefficient storing the data.\n\nCreating Database\nBecause we are dealing with data, we will import the typical libraries involving data like pandas and numpy. In addition, we will import a library called sqlite3 which will let us worth with SQL as well as databases in Python.\n\nimport pandas as pd\nimport numpy as np\nimport sqlite3\n\nNow that we have imported sqlite3, we will use the connect function to create the database in the curent directory.\n\n# Creating database in current directory we named data.db\nconn = sqlite3.connect(\"data.db\")\n\nAfter we have created our database, we now want to create its tables, which corresponds to the csv files that we are working with. For our example, we will create three tables that will be labeled temperatures, stations, countries.\nNext, we want to read in out data. Because we have a large data set, instead of reading it in all at once, we want to read in chunks of the data one at a time. We will do this by using the keyword chunksize, which returns an iterator that reads in the number of rows from the data equal to chucksize once we start querying the iterator. It should look something like follows:\n\ndf_iter = pd.read_csv(\"temps.csv\", chunksize = 100000)\ndf = df_iter.__next__()\ndf.head()\n\n\n\n\n\n\n\n\nID\nYear\nVALUE1\nVALUE2\nVALUE3\nVALUE4\nVALUE5\nVALUE6\nVALUE7\nVALUE8\nVALUE9\nVALUE10\nVALUE11\nVALUE12\n\n\n\n\n0\nACW00011604\n1961\n-89.0\n236.0\n472.0\n773.0\n1128.0\n1599.0\n1570.0\n1481.0\n1413.0\n1174.0\n510.0\n-39.0\n\n\n1\nACW00011604\n1962\n113.0\n85.0\n-154.0\n635.0\n908.0\n1381.0\n1510.0\n1393.0\n1163.0\n994.0\n323.0\n-126.0\n\n\n2\nACW00011604\n1963\n-713.0\n-553.0\n-99.0\n541.0\n1224.0\n1627.0\n1620.0\n1596.0\n1332.0\n940.0\n566.0\n-108.0\n\n\n3\nACW00011604\n1964\n62.0\n-85.0\n55.0\n738.0\n1219.0\n1442.0\n1506.0\n1557.0\n1221.0\n788.0\n546.0\n112.0\n\n\n4\nACW00011604\n1965\n44.0\n-105.0\n38.0\n590.0\n987.0\n1500.0\n1487.0\n1477.0\n1377.0\n974.0\n31.0\n-178.0\n\n\n\n\n\n\n\nOnce we have our table, we want to create a function that will clean our data (as we will repeat this for the other two tables). In our data, each column is representative for each month of the year. Thus, we want to use the stack() function of pandas to create one column for the month data.\n\ndef prepare_df(df):\n    df = df.set_index(keys=[\"ID\", \"Year\"])\n    df = df.stack()\n    df = df.reset_index()\n    df = df.rename(columns = {\"level_2\"  : \"Month\" , 0 : \"Temp\"})\n    df[\"Month\"] = df[\"Month\"].str[5:].astype(int)\n    df[\"Temp\"]  = df[\"Temp\"] / 100\n    return(df)\n\nWith our function, we are ready to read in and populate the temperature table in our database. To do this, we want to use df.to_sql() which will write to the specified table (in our case, the conn object we created earlier). In addition to make sure each piece is added to the table and nothing is overwritten after each iteration, we will use if_exists.\n\n# Reading in data for the temperature table\nfor df in df_iter:\n    df = prepare_df(df)\n    df.to_sql(\"temperatures\", conn, if_exists = \"append\", index = False)\ndf.head()\n\n\n\n\n\n\n\n\nID\nYear\nMonth\nTemp\n\n\n\n\n0\nUSW00014924\n2016\n1\n-13.69\n\n\n1\nUSW00014924\n2016\n2\n-8.40\n\n\n2\nUSW00014924\n2016\n3\n-0.20\n\n\n3\nUSW00014924\n2016\n4\n3.21\n\n\n4\nUSW00014924\n2016\n5\n13.85\n\n\n\n\n\n\n\nAnd with that we have finished the temperature table of our database. Now, we want to repeat this for the station table.\n\n# stations table \nurl = \"station-metadata.csv\"\nstations = pd.read_csv(url)\nstations.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\n\n\n\n\n\n\n\nNote with this current data set, there is no common column between our countries data and stations data. Thus, to match the two, we will add a new column to this data frame labeled CODE and represents the first two letters of the ID column.\n\nstations[\"CODE\"] = stations[\"ID\"].str[:2]\nstations.head()\n\n\n\n\n\n\n\n\nID\nLATITUDE\nLONGITUDE\nSTNELEV\nNAME\nCODE\n\n\n\n\n0\nACW00011604\n57.7667\n11.8667\n18.0\nSAVE\nAC\n\n\n1\nAE000041196\n25.3330\n55.5170\n34.0\nSHARJAH_INTER_AIRP\nAE\n\n\n2\nAEM00041184\n25.6170\n55.9330\n31.0\nRAS_AL_KHAIMAH_INTE\nAE\n\n\n3\nAEM00041194\n25.2550\n55.3640\n10.4\nDUBAI_INTL\nAE\n\n\n4\nAEM00041216\n24.4300\n54.4700\n3.0\nABU_DHABI_BATEEN_AIR\nAE\n\n\n\n\n\n\n\nNow, we can create and fill the stations table of the database using the same to_sql() method.\n\n# Creating the stations table\nstations.to_sql(\"stations\", conn, if_exists = \"replace\", index = False)\n\n27585\n\n\nWe will repeat this process one more time for our countries table now.\n\n# Creating the countries table\nurl = \"countries.csv\"\ncountries = pd.read_csv(url)\ncountries.to_sql(\"countries\", conn, if_exists = \"replace\", index = False)\n\n279\n\n\nNow we have a database containing three tables. Let’s just check that this is indeed the case.\n\ncursor = conn.cursor()\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\nprint(cursor.fetchall())\n\n[('temperatures',), ('stations',), ('countries',)]\n\n\nGreat! We officially have our three tables.\n\n\nWriting Query Function\nWe are now ready to construct the query function. First, let’s examine what type of data we are working with for each table.\n\ncursor = conn.cursor()\ncursor.execute(f\"SELECT sql FROM sqlite_master WHERE type='table';\")\n\nfor result in cursor.fetchall():\n    print(result[0])\n\nCREATE TABLE \"temperatures\" (\n\"ID\" TEXT,\n  \"Year\" INTEGER,\n  \"Month\" INTEGER,\n  \"Temp\" REAL\n)\nCREATE TABLE \"stations\" (\n\"ID\" TEXT,\n  \"LATITUDE\" REAL,\n  \"LONGITUDE\" REAL,\n  \"STNELEV\" REAL,\n  \"NAME\" TEXT,\n  \"CODE\" TEXT\n)\nCREATE TABLE \"countries\" (\n\"FIPS 10-4\" TEXT,\n  \"ISO 3166\" TEXT,\n  \"Name\" TEXT\n)\n\n\nNote that the temperatures and stations tables share the ID column while the stations and countries tables have matching entries under the CODE and FIPS 10-4, respectively\nWith this information, we want to create a function that will incorporate SQL in order to select the desired output data as well as matching up the tables and implement specified conditions. It should look something like below:\n\nfrom climate_database import query_climate_database\nimport inspect\nprint(inspect.getsource(query_climate_database))\n\ndef query_climate_database(db_file, country, year_begin, year_end, month):\n    with sqlite3.connect(db_file) as conn:\n        cmd = \\\n        f\"\"\"\n        SELECT S.NAME, S.LATITUDE, S.LONGITUDE, C.Name, T.Year, T.Month, T.Temp\n        FROM temperatures T\n        INNER JOIN stations S ON T.id = S.id\n        INNER JOIN countries C ON SUBSTR(S.id, 1, 2) = C.[FIPS 10-4]\n        WHERE C.Name = ? AND T.Year BETWEEN ? AND ? AND T.Month = ?\n        \"\"\"\n\n        df = pd.read_sql_query(cmd, conn,params = (country, year_begin, year_end, month))\n        df = df.rename(columns = {\"Name\"  : \"Country\"})\n        return df\n\n\n\nNow we have our query function. Let’s see if it is outputting our desired data set using a randon test case. For our example, we will to retrieve data from India in the month of January from 1980 to 2020.\n\nquery_climate_database(db_file = \"data.db\",\n                       country = \"India\", \n                       year_begin = 1980, \n                       year_end = 2020,\n                       month = 1)\n\n\n\n\n\n\n\n\nNAME\nLATITUDE\nLONGITUDE\nCountry\nYear\nMonth\nTemp\n\n\n\n\n0\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1980\n1\n23.48\n\n\n1\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1981\n1\n24.57\n\n\n2\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1982\n1\n24.19\n\n\n3\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1983\n1\n23.51\n\n\n4\nPBO_ANANTAPUR\n14.583\n77.633\nIndia\n1984\n1\n24.81\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n47275\nDARJEELING\n27.050\n88.270\nIndia\n1983\n1\n5.10\n\n\n47276\nDARJEELING\n27.050\n88.270\nIndia\n1986\n1\n6.90\n\n\n47277\nDARJEELING\n27.050\n88.270\nIndia\n1994\n1\n8.10\n\n\n47278\nDARJEELING\n27.050\n88.270\nIndia\n1995\n1\n5.60\n\n\n47279\nDARJEELING\n27.050\n88.270\nIndia\n1997\n1\n5.70\n\n\n\n\n47280 rows × 7 columns\n\n\n\nGreat! Our query function is returnning exactly what we wanted.\n\n\nWriting Geographic Scatter Function\nWith our basic query function down, we will now diverge into different examples of them. The first example we will make is a scatter function which uses data to create scatterplot.\nFor this example, we will be using the plotly library which allows us to create interactive data visualization, so we will need to import that.\n\nfrom plotly import express as px\nimport plotly.io as pio\npio.renderers.default=\"iframe\"\n\nFirst, we need to create a coeff() function that will perform linear linear regression on the data, using the years as the independent variable and the temperatures as the dependent variable. Because we are dealing with linear regression, we will also need to import it from the sklearn library. Also, in order to read the numbers as months, we will import calendar.\n\nfrom sklearn.linear_model import LinearRegression\nimport calendar\n\ndef coef(data_group):\n    x = data_group[[\"Year\"]] # Creating a dataframe consisting of the years\n    y = data_group[\"Temp\"]   # Creating a string of the temperatures\n    LR = LinearRegression()\n    LR.fit(x, y)\n    return LR.coef_[0]\n\nWith this function, we can now create a function, which we will call temperature_plot() to create our plots. Because we already have a query function to create our desired dataframe, we are going to call it in our function to make it simpler.\n\ndegree_sign = u'\\N{DEGREE SIGN}' # Syntax for the degree symbol\n\ndef temperature_coefficient_plot(country, year_begin, year_end, month, min_obs, **kwargs):\n    '''\n    This function creates a scatterplot figure for data on yearly temperature \n    increase for constraints specified by the user's parameter inputs.\n    \n    Parameters:\n    ----------\n    country: string representing the country name that is returned\n    year_begin: integer representing the earliest year the output includes\n    year_end: integeger representing the latest year the output includes\n    month: integer representing the month of the year that is returned\n    min_obs: integer representing the minimum required number of years of data for any given station\n    \n    Return:\n    ----------\n    A plotly geographic scatterplot involving the different stations and their annual\n    temperature increase given a specified country, year range, and month of the year.\n    '''\n    df = query_climate_database(\"data.db\", country, year_begin, year_end, month) # Creating our desired dataframe to work on\n    df[\"Station_Counts\"] = df.groupby(\"NAME\")['NAME'].transform('count') # Creating new column to track the number of occurance of a given station\n    df = df[df[\"Station_Counts\"] &gt;= min_obs] # Creating dataframe of data satisfying minimum number of station appearance\n        \n    coeff = df.groupby([\"NAME\",\"LATITUDE\",\"LONGITUDE\",\"Country\",\"Month\", \"Station_Counts\"]).apply(coef) # Calculating annual change in temperature\n    coeff = coeff.reset_index() # Reformatting dataframe\n    coeff = coeff.rename(columns = {0 : f\"Estimated Yearly Increase ({degree_sign}C)\"}) # Renaming column\n    \n    coeff[f\"Estimated Yearly Increase ({degree_sign}C)\"] = coeff[f\"Estimated Yearly Increase ({degree_sign}C)\"].round(4) # Setting number of sig figs\n    \n    return px.scatter_mapbox(coeff,\n                        lat = \"LATITUDE\",\n                        lon = \"LONGITUDE\",\n                        hover_name = \"NAME\",\n                        color = f\"Estimated Yearly Increase ({degree_sign}C)\",\n                        title = f\"Estimates of Yearly Increase in {calendar.month_name[month]} for Stations in {country} ({year_begin} - {year_end})\",\n                        **kwargs)\n\n\ncolor_map = px.colors.diverging.RdGy_r # Selecting colormap\n\nfig = temperature_coefficient_plot(\"India\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\nLet’s also see how the United States looks like with the same time settings!\n\ncolor_map = px.colors.diverging.RdGy_r # Selecting colormap\n\nfig = temperature_coefficient_plot(\"United States\", 1980, 2020, 1, \n                                   min_obs = 10,\n                                   zoom = 2,\n                                   mapbox_style=\"carto-positron\",\n                                   color_continuous_scale=color_map)\n\nfig.show()\n\n\n\n\n\n\nMore Figures / Data Visualisations\nTo end, we will create two more plots. Because our data involves temperature, we will create figures based on climate comparisons.\nBecause a common researched aspect of climate change is highly variable weather (meaning it is expected that weather of both extremes become more similar as time progresses), we will first create a plot looking at countries’ variance in temperature over time across its stations. Again, we can call the query function to create our dataframe.\n\ndef temperature_variance_plot(country, year_begin, year_end, month, min_obs, **kwargs):\n    \"\"\"\n    This function creates a scatterplot figure for data on yearly temperature standard \n    deviation change for all the stations of a country specified by the user.\n    \n    Parameters:\n    ----------\n    country: string representing the country name that is returned\n    year_begin: integer representing the earliest year the output includes\n    year_end: integeger representing the latest year the output includes\n    month: integer representing the month of the year that is returned\n    min_occ: integer representing the minimum required number of years of data for any given station\n    \n    Return:\n    ----------\n    A plotly geographic scatterplot involving the annal tempearture standard deviation change\n    given a specified country, year range, and month of the year.\n    \"\"\"\n    \n    df = query_climate_database(\"data.db\", country, year_begin, year_end, month) # Creating our desired dataframe to work on\n    \n    df[\"Station_Counts\"] = df.groupby(\"NAME\")['NAME'].transform('count') # Creating new column to track the number of occurance of a given station\n    df = df[df[\"Station_Counts\"] &gt;= min_obs] # Creating dataframe of data satisfying minimum number of station appearance\n    \n    SD = df.groupby([\"NAME\",\"Year\"])[\"Temp\"].std() # Calculating annual standard deviation\n    SD = SD.reset_index() #Reformatting data frame\n    \n    coeff = df.groupby([\"NAME\"]).apply(coef)  # Calculating annual change in standard deviation\n    coeff = coeff.reset_index() # Reformatting dataframe\n    coeff = coeff.rename(columns = {0 : \"Correlation Coeff R\"}) # Renaming column\n    \n    \n    return px.scatter(data_frame = coeff,   \n                 y = \"Correlation Coeff R\", \n                 title = f\"Annual Standard Deviation Changes of Stations in {country}({year_begin} - {year_end})\",\n                 hover_name = \"NAME\",\n                 hover_data = [\"Correlation Coeff R\"])\n\nWith this function, let’s look into the variance of the US’s temperature in March from 1980 to 2020\n\nfig = temperature_variance_plot(\"United States\", 1980, 2020, 3, min_obs = 3)\nfig.show()\n\n\n\n\nFrom this figure, we can see that because the R-value, representing the average annual change, is mostly around 0, it means that there has been no positive or negative trend in terms of temperature variance. Note that this is for the US only, which means this trend should not be applied to other countries as well.\nFor our second one, let’s examine the climate/weather of two geographically similar and close countries. For this, we will need to use histograms to look at their temperature distributions. Unlike the other two, since we want our dataframe to incorporate two different countries, we will have to use a different cmd.\n\ndef temperature_histogram_plot(country1, country2, year_begin, year_end, **kwargs):\n    \"\"\"\n    This function generates two histograms of countries' \n    temperature distribution specified year range\n    \n    Parameters:\n    ----------\n    country1: string giving the name of the first country \n    country2: string giving the name of the second country\n    year_begin: integer representing the earliest year the output includes\n    year_end: integer representing the latest year the output includes\n    \n    Return:\n    ----------\n    A plotly normalized histograms\n    \"\"\"\n    \n    cmd = \\\n    f\"\"\"\n    SELECT S.name, S.latitude, S.longitude, C.name, T.year, T.month, T.temp\n    FROM temperatures T\n    LEFT JOIN stations S ON T.id = S.id\n    LEFT JOIN countries C ON S.code = C.\"FIPS 10-4\"\n    WHERE C.name = \"{country1}\" OR C.name = \"{country2}\" AND T.year BETWEEN {year_begin} AND {year_end}\n    \"\"\"\n    \n    df = pd.read_sql_query(cmd, conn)\n    df = df.rename(columns = {\"Name\" : \"Country\"})\n    \n    return px.histogram(df,\n                  x = \"Temp\",\n                  opacity = 0.5,\n                  nbins = 30,\n                  histnorm = \"percent\", # Normalizing histograms in case the countries are largely different in counts\n                  barmode = \"stack\", \n                  facet_col = \"Country\",\n                  labels={\n                     \"Temp\": f\"Temperature ({degree_sign}C)\",\n                     \"count\": \"Normalized Counts\"\n                  },\n                  title = f\"Normalized Temperature Distributions of {country1} and {country2} from years {year_begin} - {year_end}\",\n                  **kwargs)\n\nNote that this is creating normalized histogram distibutions to prevent unfair visual comparison. Now we can test our function using two close countries, like India and Pakistan.\n\nfig = temperature_histogram_plot(\"India\", \"Pakistan\", 1980, 2020)\nfig.show()\n\n\n\n\nFrom this figure, we can see how India has a a larger temperature range, from -8 to 38 degrees Celsius, compared to Pakistan, from 0 to 38 degrees. In addition, it also has a higher peek than Pakistan. However, Pakistan’s peak is father away than India, peaking at 30 to 32 degrees while India peaked at 26-28 degrees. We can also not that both left-skewed. Thus, while there is a similarity between the two countries, there are also major differences in terms of climate. This makes sense since India is a much more geographically large countyry compared to Pakistan, which would mean it should hve more variance in temperature across different stations. In addition, India has more coastal regions and also takes up majority of the Himilayas mountains, resulting in colder temperatures than Pakistan which has less coastal regions and only takes up five percent of the mountains.\nNow that we have created all our figures, let’s make sure to close the database connection.\n\nconn.close()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "pic16bblog",
    "section": "",
    "text": "Homework 3: Web Development\n\n\n\n\n\n\nweek 6\n\n\nhomework\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 2: Web Scraping (Movie Recs)\n\n\n\n\n\n\nWeek 5\n\n\nHomework\n\n\n\n\n\n\n\n\n\nFeb 7, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 0: Data Visualization\n\n\n\n\n\n\nWeek 1\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nHomework 1: Database Visualization (Climate Change)\n\n\n\n\n\n\nWeek 2\n\n\nHomework\n\n\n\n\n\n\n\n\n\nJan 19, 2024\n\n\nThomas Nguyen\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 14, 2024\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/bruin/index.html",
    "href": "posts/bruin/index.html",
    "title": "Homework 0: Data Visualization",
    "section": "",
    "text": "In this blog, I will use the Palmer Penguins dataset to show how to create interesting data visualization. There are many visuals that we can use for the data set like scatterplot, histogram, or boxplot, but for today, we will use scatterplots.\n\nPrepping Dataframe\nTo access the dataframe, we first have to read in the csv file using pandas. Thus, we need to import the pandas library in order to work with data.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/pic16b-ucla/24W/main/datasets/palmer_penguins.csv\"\npenguins = pd.read_csv(url)     \n\nNext, we want to visualize the data structure to see what type of information we are working with. We can use the head function of pandas to access the first five entries.\n\npenguins.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0708\n1\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A1\nYes\n11/11/07\n39.1\n18.7\n181.0\n3750.0\nMALE\nNaN\nNaN\nNot enough blood for isotopes.\n\n\n1\nPAL0708\n2\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN1A2\nYes\n11/11/07\n39.5\n17.4\n186.0\n3800.0\nFEMALE\n8.94956\n-24.69454\nNaN\n\n\n2\nPAL0708\n3\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A1\nYes\n11/16/07\n40.3\n18.0\n195.0\n3250.0\nFEMALE\n8.36821\n-25.33302\nNaN\n\n\n3\nPAL0708\n4\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN2A2\nYes\n11/16/07\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nAdult not sampled.\n\n\n4\nPAL0708\n5\nAdelie Penguin (Pygoscelis adeliae)\nAnvers\nTorgersen\nAdult, 1 Egg Stage\nN3A1\nYes\n11/16/07\n36.7\n19.3\n193.0\n3450.0\nFEMALE\n8.76651\n-25.32426\nNaN\n\n\n\n\n\n\n\nFollowing standard pandas operations, we want to clean up our data a little bit, removing any “N/A” entries and shortening the names.\n\npenguins = penguins.dropna(subset = [\"Body Mass (g)\", \"Sex\"])  # Removing penguins with NaN entries in Sex or Body Mass\npenguins[\"Species\"] = penguins[\"Species\"].str.split().str.get(0)   # Only getting the first word of the Species entry (i.e. species type)\ncols = [\"Species\",                 # Specifying the columns we want to look further\n        \"Island\",                  # into from the penguins data set\n        \"Sex\", \n        \"Culmen Length (mm)\", \n        \"Culmen Depth (mm)\", \n        \"Flipper Length (mm)\", \n        \"Body Mass (g)\"]\npenguins = penguins[cols]          # Choosing the columns we want our data frame to consist of\n\nNow, let’s take a look at the new simplified data set.\n\npenguins.head()\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n\n\n\n\n\nNote that there were additional column information which we are going to ignore for today.\nOnce the data set is cleaned, we can start creating our data visualizations.\n\n\nSeparating Dataframe\nBecause this data set consists of different types of penguin species, we need to figure out how many species types we are dealing with and what their names are so that we know what to call in our code. We can use the value_counts function in the pandas library to do the following:\n\npenguins[\"Species\"].value_counts()  # Counting the number of penguins be each Species type\n\nSpecies\nAdelie       146\nGentoo       120\nChinstrap     68\nName: count, dtype: int64\n\n\nFrom this, we know that the penguins in this data set are classified as one of the three following species type: Adelie, Gentoo, or Chinstrap.\nNow, we can start writing our code to create the scatterplot. First, we need to separate the penguin types and create individual data frames for each of the three species. To do so, we can use boolean indexing. We can use a == statement to return a True or False value indicating whether or not a specific entry is of the indicated penguin species\nFor example, to find only the Gentoo penguins, we will say penguins[\"Species\"]==\"Gentoo\" as our conditional statement for including an into the created data frame. This pattern will help us create our three separate data frames as follows:\n\n# Generating the separate dataframes based on Species\nAdelie_dataset = penguins[penguins[\"Species\"] == \"Adelie\"]\nGentoo_dataset = penguins[penguins[\"Species\"] == \"Gentoo\"]\nChinstrap_dataset = penguins[penguins[\"Species\"] == \"Chinstrap\"]\n\nLet’s check to see if this successfully separated the penguins by species.\n\nAdelie_dataset\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n0\nAdelie\nTorgersen\nMALE\n39.1\n18.7\n181.0\n3750.0\n\n\n1\nAdelie\nTorgersen\nFEMALE\n39.5\n17.4\n186.0\n3800.0\n\n\n2\nAdelie\nTorgersen\nFEMALE\n40.3\n18.0\n195.0\n3250.0\n\n\n4\nAdelie\nTorgersen\nFEMALE\n36.7\n19.3\n193.0\n3450.0\n\n\n5\nAdelie\nTorgersen\nMALE\n39.3\n20.6\n190.0\n3650.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n147\nAdelie\nDream\nFEMALE\n36.6\n18.4\n184.0\n3475.0\n\n\n148\nAdelie\nDream\nFEMALE\n36.0\n17.8\n195.0\n3450.0\n\n\n149\nAdelie\nDream\nMALE\n37.8\n18.1\n193.0\n3750.0\n\n\n150\nAdelie\nDream\nFEMALE\n36.0\n17.1\n187.0\n3700.0\n\n\n151\nAdelie\nDream\nMALE\n41.5\n18.5\n201.0\n4000.0\n\n\n\n\n146 rows × 7 columns\n\n\n\n\nGentoo_dataset\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n220\nGentoo\nBiscoe\nFEMALE\n46.1\n13.2\n211.0\n4500.0\n\n\n221\nGentoo\nBiscoe\nMALE\n50.0\n16.3\n230.0\n5700.0\n\n\n222\nGentoo\nBiscoe\nFEMALE\n48.7\n14.1\n210.0\n4450.0\n\n\n223\nGentoo\nBiscoe\nMALE\n50.0\n15.2\n218.0\n5700.0\n\n\n224\nGentoo\nBiscoe\nMALE\n47.6\n14.5\n215.0\n5400.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n338\nGentoo\nBiscoe\nFEMALE\n47.2\n13.7\n214.0\n4925.0\n\n\n340\nGentoo\nBiscoe\nFEMALE\n46.8\n14.3\n215.0\n4850.0\n\n\n341\nGentoo\nBiscoe\nMALE\n50.4\n15.7\n222.0\n5750.0\n\n\n342\nGentoo\nBiscoe\nFEMALE\n45.2\n14.8\n212.0\n5200.0\n\n\n343\nGentoo\nBiscoe\nMALE\n49.9\n16.1\n213.0\n5400.0\n\n\n\n\n120 rows × 7 columns\n\n\n\n\nChinstrap_dataset\n\n\n\n\n\n\n\n\nSpecies\nIsland\nSex\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\n\n\n\n\n152\nChinstrap\nDream\nFEMALE\n46.5\n17.9\n192.0\n3500.0\n\n\n153\nChinstrap\nDream\nMALE\n50.0\n19.5\n196.0\n3900.0\n\n\n154\nChinstrap\nDream\nMALE\n51.3\n19.2\n193.0\n3650.0\n\n\n155\nChinstrap\nDream\nFEMALE\n45.4\n18.7\n188.0\n3525.0\n\n\n156\nChinstrap\nDream\nMALE\n52.7\n19.8\n197.0\n3725.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n215\nChinstrap\nDream\nMALE\n55.8\n19.8\n207.0\n4000.0\n\n\n216\nChinstrap\nDream\nFEMALE\n43.5\n18.1\n202.0\n3400.0\n\n\n217\nChinstrap\nDream\nMALE\n49.6\n18.2\n193.0\n3775.0\n\n\n218\nChinstrap\nDream\nMALE\n50.8\n19.0\n210.0\n4100.0\n\n\n219\nChinstrap\nDream\nFEMALE\n50.2\n18.7\n198.0\n3775.0\n\n\n\n\n68 rows × 7 columns\n\n\n\nGreat! Now all we have left is the scatterplot.\n\n\nCreating Visualization\nIn our examples, we will be using the matplotlib, a common library used for data visualizaiton. So, we need to import the matplotlib library.\n\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nFor matplotlib, the syntax for creating a scatterplot using a dataset is plt.scatter(df[x],df[y]) where df is the dataset name and x,y are the independent and dependent columns, respectively, that we want to work with. For our example, we will use Culmen Length and Flipper Length. So using Adelie, we will use the following line of code:\nplt.scatter(Adelie_dataset[\"Culmen Length (mm)\"], Adelie_dataset[\"Flipper Length (mm)\"])\nBecause we are also working with three different data frames, meaning we will have three separate scatterplots on one plot, we can also add a label in our line of code. This way, when we combine all of them into one figure, we can use a legend to help us differentiate each data set. Thus, our code for each species data set will look like this:\n\n# Generating the scatterplots for each species, labeling each scatterplot by their name so we can differentiate them with a legend\nplt.scatter(Adelie_dataset[\"Culmen Length (mm)\"], Adelie_dataset[\"Flipper Length (mm)\"], label = \"Adelie\")\nplt.scatter(Gentoo_dataset[\"Culmen Length (mm)\"], Gentoo_dataset[\"Flipper Length (mm)\"], label = \"Gentoo\")\nplt.scatter(Chinstrap_dataset[\"Culmen Length (mm)\"], Chinstrap_dataset[\"Flipper Length (mm)\"], label = \"Chinstrap\")\n\n\n\n\n\n\n\n\nNow that we have out scatterplot, we can add features to make the graph more clear. To add the main title, we can use plt.title(). For the the x and y titles, it will be plt.xlabel() and plt.ylabel(), respectively. And finally, to create our legend, we will use plt.legend(). Once we added all this, our figure may look something like this:\n\n# Generating the scatterplots for each species, labeling each scatterplot by their name so we can differentiate them with a legend\nplt.scatter(Adelie_dataset[\"Culmen Length (mm)\"], Adelie_dataset[\"Flipper Length (mm)\"], label = \"Adelie\")\nplt.scatter(Gentoo_dataset[\"Culmen Length (mm)\"], Gentoo_dataset[\"Flipper Length (mm)\"], label = \"Gentoo\")\nplt.scatter(Chinstrap_dataset[\"Culmen Length (mm)\"], Chinstrap_dataset[\"Flipper Length (mm)\"], label = \"Chinstrap\")\n\n# Adding the title, x and y axes labels, and legend\nplt.title(\"Culmen Length vs. Flipper Length\")\nplt.xlabel(\"Culmen Length (mm)\")\nplt.ylabel(\"Flipper Length (mm)\")\nplt.legend()\n\n\n\n\n\n\n\n\nWith this completed scatterplot, we can finally analyze the two data columns. From this, it appears that for all species, there is a moderate to strong positive correlation between Culmen Length and Flipper Length. Also, it appears that these two may be good candidate factors in differentiating the species type as there are three relatively isolated clusters. And with that, we have successfully created our data visualization."
  },
  {
    "objectID": "posts/HW2/index.html",
    "href": "posts/HW2/index.html",
    "title": "Homework 2: Web Scraping (Movie Recs)",
    "section": "",
    "text": "In this blog, we will learn how to extract data from HTML sites using webscraping. For our example, we will use movie sites.\n\nCreating Scrapy Project\nWe will first start by running these two commands in the terminal:\nscrapy startproject TMDB_scraper\ncd TMDB_scraper\nRunning these lines will allow us to create a folder labeled “TBDB_scraper” which we will then for this webscraping tutorial.\nInside the inner “TMBD_scraper” will exists a python file called settings.py which we will want to add these two lines:\nUSER_AGENT = \"Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148\"\nDOWNLOAD_DELAY = 2\nAdding this line will be crucial for the 403 Forbidden errors, which commonly arises since these websites have detectors that see that we are scraping the website, so they want to block us from this action.\nThe first line USER_AGENT helps implement a fake user agent for all the requests that we will send, which makes it harder for the website to know if the requests are from a scraper (which it will want to block) or an actual user.\nThe second line DOWNLOAD_DELAY helps implement a randomization of request delays, which spaces out the requests over longer, patternless intervals to make the website flag us less frequently for scraping.\n\n\nImplementing Scraping Methods\nNow that we have our scrapy project, we want to create a tmdb_spider.py file in the spiders folder.\nWe first start by importing scrapy, which is the webscraping library which will allow us to do all of this.\n\nimport scrapy\n\nNext, we need to create the spider class TmbdSpider which contains its name (tmbd_spider) that we will need to perform the webscraping along with the url for the movie from the website we are working with and its three parse methods. For this example, we will use The Dark Knight with the data from The Movie Dtabase. So, our code will look something like this:\nclass TmdbSpider(scrapy.Spider):\n    name = 'tmdb_spider'\n    def __init__(self, subdir = None, *args, **kwargs):\n        self.start_urls = [f\"https://www.themoviedb.org/movie/{subdir}/\"]\nAnd for the three parsing methods:\ndef parse(self, response):\n      \"\"\"\n      This method starts with the start_url which is a movie page on the website,\n      and proceeds by sending a request to the cast page of the movie from the start_url.\n      This has no outputs but sets up the request to be used in the parse_full_credits method.\n      \"\"\"\n        page = response.url + \"/cast\" # Adding on /cast to get the cast page of movie's url\n        yield scrapy.Request(page, callback = self.parse_full_credits)\nThe parse method starts with the start_url and then sends a request for the movie’s cast page from the start_url, which will be used in the next defined method, parse_full_credits\ndef parse_full_credits(self, response):\n      \"\"\"\n      This method starts with the cast page of a movie given through the start_url \n      from parse function. It then iterates through all the actors's, actresses's links \n      on this page and sends a request for each actor's, actress's page. This\n      produces no outputs but sets up the requests to be used in the parse_actor_page method.\n      \"\"\"\n        for entry in response.css(\"ol\")[0].css(\"li\"):\n            actor_tag = entry.css(\"a::attr(href)\").get() # Gets unique tag for each actor, actress\n            actor_link = \"https://www.themoviedb.org\" + actor_tag # Adding on tag to main website \n                                                                  # address to get the link to\n                                                                  # each actor's, actress's page  \n            yield scrapy.Request(actor_link, callback = self.parse_actor_page)\n                                           \nSimilar to the last method, this method starts with the cast page of the desired movie and then iterates through all the actor’s, actress’s links on this page and sends a request for each actor’s actress’s page, which will be used in the last defined method, parse_actor_page.\ndef parse_actor_page(self, response):\n        \"\"\"\n        This method starts with the actor/actress's page given by the \n        parse_full_credits function. It then iterates through all the movies that the \n        actor/actress has made an appearance. For each movie, the output will be\n        a dictionary where the key is thename of the actor, actress, and the value\n        is the name of the movie.\n        \"\"\" \n        actor_name = response.css(\"h2\").css(\"::text\").get() # Gets the name of actor,actress \n        for entry in response.css(\"div.credits_list bdi::text\"):\n            movie_or_TV_name = entry.get() # Iterates through each movie appearance\n            yield {\"actor\" : actor_name, \"movie_or_TV_name\" : movie_or_TV_name} # Returns dictionary\nThis method starts with the page of a certain actor, actress and iterates through all movies the this actor, actress has made an appearance and for each one movie we will return a dictionary with the their name and the movie’s name.\nGreat, now we can use our defined spider! So in terminal, navigate to the TMDB_scraper folder by running\nscrapy crawl tmdb_spider -o results.csv\nwhich will return us a csv file with the completed list of all the movies that all the actors, actresses in our chosen movie has made an appearance in.\n\n\nCreating Movie Recommender\nNow that we have our completed data set, we will want to create an algorith to help us with movie recommendations.\nWe first start off by reading in our data, so we need to import the pandas and numpy libraries.\n\nimport pandas as pd\nimport numpy as np\nresults = pd.read_csv(\"results.csv\")\nresults\n\n\n\n\n\n\n\n\nactor\nmovie_or_TV_name\n\n\n\n\n0\nHeath Ledger\nHeath Ledger: A Tragic Tale\n\n\n1\nHeath Ledger\nJoker: Put on a Happy Face\n\n\n2\nHeath Ledger\nI Am Heath Ledger\n\n\n3\nHeath Ledger\nThe Fire Rises: The Creation and Impact of The...\n\n\n4\nHeath Ledger\nToo Young to Die\n\n\n...\n...\n...\n\n\n4144\nMichael Caine\nThe Double\n\n\n4145\nMichael Caine\nBlue Ice\n\n\n4146\nMichael Caine\nThe Fourth Protocol\n\n\n4147\nMichael Caine\nPulp\n\n\n4148\nMichael Caine\nGet Carter\n\n\n\n\n4149 rows × 2 columns\n\n\n\nAs verification, we want to make sure that the number of actors and actresses listed in our data equals the size of the movie cast. So, we would run the code below:\n\nlen(results[\"actor\"].unique())\n\n136\n\n\nNow, we want to create a 2-dimensional list with one column consists of all the movie names the actors or actresses have appeared in while the other column consists of the number of actor and actresses from The Dark Knight that was also in a certain movie.\n\nmovies_or_TV_shows = results[\"movie_or_TV_name\"].unique() # Creating df containing all \n                                                          # movie names with 0 repeats\n\nrows, cols = (len(movies_or_TV_shows), 2) \nrec_list = [[0 for i in range(cols)] for j in range(rows)] # Creating 2D lists, each unique movie \n                                                           # gets in own row\n\nindex = 0\nfor movie_or_TV_show in movies_or_TV_shows:\n    \"\"\"\n    Iterating through each movie, creating a dataframe of all actors and actreses from \n    The Dark Knight movie that also appeared in the specific movie, and then counting\n    the number of actors and actresses\n    \"\"\"\n    \n    panda = results[results[\"movie_or_TV_name\"] == movie_or_TV_show]\n    num_shared_actors = len(panda)\n    \n    rec_list[index][0] = movie_or_TV_show   # Movie name list\n    rec_list[index][1] = num_shared_actors  # Counting list\n    \n    index += 1 # Updating list\n\nWith this list, we now need to sort it, which we can do using the sorted() function and lambda functions. Because we want the top movie recommendations first, we will use the reverse=True statement. Lastly, we want to convert this to a pandas dataframe to use later.\n\nrec_list_sorted = sorted(rec_list,key=lambda l:l[1], reverse=True)\nrec_list_sorted = pd.DataFrame(rec_list_sorted)\nrec_list_panda = rec_list_sorted.rename(columns = {0: \"Movie\", \n                                                   1: \"No. Actors Appearing in Dark Knight\"})\nrec_list_panda\n\n\n\n\n\n\n\n\nMovie\nNo. Actors Appearing in Dark Knight\n\n\n\n\n0\nThe Dark Knight\n138\n\n\n1\nThe Fire Rises: The Creation and Impact of The...\n10\n\n\n2\nThe Dark Knight Rises\n9\n\n\n3\nCSI: Crime Scene Investigation\n9\n\n\n4\nDoctor Who\n9\n\n\n...\n...\n...\n\n\n3383\nTony Awards\n1\n\n\n3384\nNavy Log\n1\n\n\n3385\nMark Saber\n1\n\n\n3386\nWhat's My Line?\n1\n\n\n3387\nMorning Departure\n1\n\n\n\n\n3388 rows × 2 columns\n\n\n\nNote that for the first entry, “The Dark Knight” as a count of 138 when we were expecting to only get 136. This is because two actors, Heath Ledger and Tom McComas, are actually listed twice on their cast page, which accounts for the increase of two counts.\nGreat! Now we have our ordered list! Ignoring “The Dark Knight” with the highest count (which is a given), we can see that the next highest is “The Fire Rises: The Creation and Impact of the Dark Knight Trilogy,” a documentary about “The Dark Knight” trilogy (which also makes sense), so we shold ignore that one as well. The third highest is “The Dark Knight Rises,” followed by “CSI: Crime Scene Investigation” and then “Doctor Who.” These are our top three recommendations\n\n\nCreating Data Visualization\nTo end, let’s create a data visualization showing a bar chart of number of shared actors for the movies.\nBecause there over 3,000 movies, let’s restrict the amount we want to include by imposing a minimum count. For this example, we will make it 6. Also, because we only want recommendations, we will exclude The Dark Knight in the visualization as well.\nFirst we want to create our considered movie rec dataframe.\n\nimport matplotlib.pyplot as plt \nbar_data = pd.DataFrame(rec_list_sorted)             # Creating dataframe of 2D sorted list\nbar_data = bar_data[bar_data[1] &lt; max(bar_data[1])]  # Removing The Dark Knight entry\nbar_data = bar_data[bar_data[1] &gt;= 6]                # Restricting minimum score to be 6\nbar_data\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n1\nThe Fire Rises: The Creation and Impact of The...\n10\n\n\n2\nThe Dark Knight Rises\n9\n\n\n3\nCSI: Crime Scene Investigation\n9\n\n\n4\nDoctor Who\n9\n\n\n5\nGotham Tonight\n8\n\n\n6\nPrison Break\n7\n\n\n7\nWaking the Dead\n7\n\n\n8\nHeath Ledger: A Tribute\n6\n\n\n9\nE! True Hollywood Story\n6\n\n\n10\nEnding the Knight\n6\n\n\n11\nBatman Begins\n6\n\n\n12\nCasualty\n6\n\n\n13\nThe View\n6\n\n\n14\nLIVE with Kelly and Mark\n6\n\n\n\n\n\n\n\nFinally, we can create our bar chart figure. Because we have titles as our independent variable, which can be long, we want to make a horizontal bar chart instead of a vertical one.\n\nfig = plt.figure(figsize = (10, 5))\n \n# creating the bar plot\nplt.barh(bar_data[0], bar_data[1], color ='Blue')\n \nplt.xlabel(\"Movie\")\nplt.ylabel(\"No. of Actor/Actress Appearances Who Were Also in The Dark Knight\")\nplt.title(\"Bar Plot of the Top Recs for The Dark Knight\")\nplt.show()"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  }
]